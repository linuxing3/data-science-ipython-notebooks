{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! -*-coding:utf-8-*-\n",
    "# Function: 房价调查\n",
    "# Author：蘭兹\n",
    "\n",
    "from urllib import parse, request\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import lxml\n",
    "import datetime\n",
    "import cProfile\n",
    "import socket\n",
    "import copy\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "抓取完成，输入文件名保存：house.xlsx\n",
      "************************一级抓取：正在抓取【东城】************************\n",
      "************************一级抓取：正在抓取【丰台】************************\n",
      "************************一级抓取：正在抓取【朝阳】************************\n",
      "************************一级抓取：正在抓取【平谷】************************\n",
      "************************一级抓取：正在抓取【门头沟】************************\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "\n",
    "base_url = r'http://bj.fangjia.com/ershoufang/'\n",
    "\n",
    "\n",
    "test_search_dict = {'昌平': {'霍营': {'13号线': 'http://bj.fangjia.com/ershoufang/--r-%E6%98%8C%E5%B9%B3|w-13%E5%8F%B7%E7%BA%BF|b-%E9%9C%8D%E8%90%A5'}}}\n",
    "\n",
    "search_list = []  # 房源信息url列表\n",
    "tmp_list = []  # 房源信息url缓存列表\n",
    "layer = -1\n",
    "\n",
    "\n",
    "# 获取列表页面\n",
    "def get_page(url):\n",
    "    headers = {\n",
    "        'User-Agent': r'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                      r'Chrome/45.0.2454.85 Safari/537.36 115Browser/6.0.3',\n",
    "        'Referer': r'http://bj.fangjia.com/ershoufang/',\n",
    "        'Host': r'bj.fangjia.com',\n",
    "        'Connection': 'keep-alive'\n",
    "    }\n",
    "    timeout = 60\n",
    "    socket.setdefaulttimeout(timeout)  # 设置超时\n",
    "    req = request.Request(url, headers=headers)\n",
    "    response = request.urlopen(req).read()\n",
    "    page = response.decode('utf-8')\n",
    "    return page\n",
    "\n",
    "\n",
    "# 获取查询关键词dict\n",
    "def get_search(page, key):\n",
    "    soup = BS(page, 'lxml')\n",
    "    search_list = soup.find_all(href=re.compile(key), target='')\n",
    "    search_dict = {}\n",
    "    for i in range(len(search_list)):\n",
    "        soup = BS(str(search_list[i]), 'lxml')\n",
    "        key = soup.select('a')[0].get_text()\n",
    "        value = soup.a.attrs['href']\n",
    "        search_dict[key] = value\n",
    "    return search_dict\n",
    "\n",
    "\n",
    "# 获取房源信息列表(嵌套字典遍历)\n",
    "def get_info_list(search_dict, layer, tmp_list, search_list):\n",
    "    layer += 1  # 设置字典层级\n",
    "    for i in range(len(search_dict)):\n",
    "        tmp_key = list(search_dict.keys())[i]  # 提取当前字典层级key\n",
    "        tmp_list.append(tmp_key)   # 将当前key值作为索引添加至tmp_list\n",
    "        tmp_value = search_dict[tmp_key]\n",
    "        if isinstance(tmp_value, str):   # 当键值为url时\n",
    "            tmp_list.append(tmp_value)   # 将url添加至tmp_list\n",
    "            search_list.append(copy.deepcopy(tmp_list))   # 将tmp_list索引url添加至search_list\n",
    "            tmp_list = tmp_list[:layer]  # 根据层级保留索引\n",
    "        elif tmp_value == '':   # 键值为空时跳过\n",
    "            layer -= 2           # 跳出键值层级\n",
    "            tmp_list = tmp_list[:layer]   # 根据层级保留索引\n",
    "        else:\n",
    "            get_info_list(tmp_value, layer, tmp_list, search_list)  # 当键值为列表时，迭代遍历\n",
    "            tmp_list = tmp_list[:layer]\n",
    "    return search_list\n",
    "\n",
    "\n",
    "# 获取房源信息详情\n",
    "def get_info_pn_list(search_list):\n",
    "    fin_search_list = []\n",
    "    for i in range(len(search_list)):\n",
    "        print('>>>正在抓取%s' % search_list[i][:3])\n",
    "        search_url = search_list[i][3]\n",
    "        try:\n",
    "            page = get_page(search_url)\n",
    "        except:\n",
    "            print('获取页面超时')\n",
    "            continue\n",
    "        soup = BS(page, 'lxml')\n",
    "        # 获取最大页数\n",
    "        pn_num = soup.select('span[class=\"mr5\"]')[0].get_text()\n",
    "        rule = re.compile(r'\\d+')\n",
    "        max_pn = int(rule.findall(pn_num)[1])\n",
    "        # 组装url\n",
    "        for pn in range(1, max_pn+1):\n",
    "            print('************************正在抓取%s页************************' % pn)\n",
    "            pn_rule = re.compile('[|]')\n",
    "            fin_url = pn_rule.sub(r'|e-%s|' % pn, search_url, 1)\n",
    "            tmp_url_list = copy.deepcopy(search_list[i][:3])\n",
    "            tmp_url_list.append(fin_url)\n",
    "            fin_search_list.append(tmp_url_list)\n",
    "    return fin_search_list\n",
    "\n",
    "\n",
    "# 获取tag信息\n",
    "def get_info(fin_search_list, process_i):\n",
    "    print('进程%s开始' % process_i)\n",
    "    fin_info_list = []\n",
    "    for i in range(len(fin_search_list)):\n",
    "        url = fin_search_list[i][3]\n",
    "        try:\n",
    "            page = get_page(url)\n",
    "        except:\n",
    "            print('获取tag超时')\n",
    "            continue\n",
    "        soup = BS(page, 'lxml')\n",
    "        title_list = soup.select('a[class=\"h_name\"]')\n",
    "        address_list = soup.select('span[class=\"address]')\n",
    "        attr_list = soup.select('span[class=\"attribute\"]')\n",
    "        price_list = soup.find_all(attrs={\"class\": \"xq_aprice xq_esf_width\"})  # select对于某些属性值（属性值中间包含空格）无法识别，可以用find_all(attrs={})代替\n",
    "        for num in range(20):\n",
    "            tag_tmp_list = []\n",
    "            try:\n",
    "                title = title_list[num].attrs[\"title\"]\n",
    "                print(r'************************正在获取%s************************' % title)\n",
    "                address = re.sub('\\n', '', address_list[num].get_text())\n",
    "                area = re.search('\\d+[\\u4E00-\\u9FA5]{2}', attr_list[num].get_text()).group(0)\n",
    "                layout = re.search('\\d[^0-9]\\d.', attr_list[num].get_text()).group(0)\n",
    "                floor = re.search('\\d/\\d', attr_list[num].get_text()).group(0)\n",
    "                price = re.search('\\d+[\\u4E00-\\u9FA5]', price_list[num].get_text()).group(0)\n",
    "                unit_price = re.search('\\d+[\\u4E00-\\u9FA5]/.', price_list[num].get_text()).group(0)\n",
    "                tag_tmp_list = copy.deepcopy(fin_search_list[i][:3])\n",
    "                for tag in [title, address, area, layout, floor, price, unit_price]:\n",
    "                    tag_tmp_list.append(tag)\n",
    "                fin_info_list.append(tag_tmp_list)\n",
    "            except:\n",
    "                print('【抓取失败】')\n",
    "                continue\n",
    "    print('进程%s结束' % process_i)\n",
    "    return fin_info_list\n",
    "\n",
    "\n",
    "# 分配任务\n",
    "def assignment_search_list(fin_search_list, project_num):  # project_num每个进程包含的任务数，数值越小，进程数越多\n",
    "    assignment_list = []\n",
    "    fin_search_list_len = len(fin_search_list)\n",
    "    for i in range(0, fin_search_list_len, project_num):\n",
    "        start = i\n",
    "        end = i+project_num\n",
    "        assignment_list.append(fin_search_list[start: end])  # 获取列表碎片\n",
    "    return assignment_list\n",
    "\n",
    "\n",
    "# 存储抓取结果\n",
    "def save_excel(fin_info_list, file_name):\n",
    "    tag_name = ['区域', '板块', '地铁', '标题', '位置', '平米', '户型', '楼层', '总价', '单位平米价格']\n",
    "    book = xlsxwriter.Workbook(r'C:\\Users\\Administrator\\Desktop\\%s.xls' % file_name)  # 默认存储在桌面上\n",
    "    tmp = book.add_worksheet()\n",
    "    row_num = len(fin_info_list)\n",
    "    for i in range(1, row_num):\n",
    "        if i == 1:\n",
    "            tag_pos = 'A%s' % i\n",
    "            tmp.write_row(tag_pos, tag_name)\n",
    "        else:\n",
    "            con_pos = 'A%s' % i\n",
    "            content = fin_info_list[i-1]  # -1是因为被表格的表头所占\n",
    "            tmp.write_row(con_pos, content)\n",
    "    book.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_name = input(r'抓取完成，输入文件名保存：')\n",
    "    fin_save_list = []  # 抓取信息存储列表\n",
    "    # 一级筛选\n",
    "    page = get_page(base_url)\n",
    "    search_dict = get_search(page, 'r-')\n",
    "    # 二级筛选\n",
    "    for k in search_dict:\n",
    "        print(r'************************一级抓取：正在抓取【%s】************************' % k)\n",
    "        url = search_dict[k]\n",
    "        second_page = get_page(url)\n",
    "        second_search_dict = get_search(second_page, 'b-')\n",
    "        search_dict[k] = second_search_dict\n",
    "    # 三级筛选\n",
    "    for k in search_dict:\n",
    "        second_dict = search_dict[k]\n",
    "        for s_k in second_dict:\n",
    "            print(r'************************二级抓取：正在抓取【%s】************************' % s_k)\n",
    "            url = second_dict[s_k]\n",
    "            third_page = get_page(url)\n",
    "            third_search_dict = get_search(third_page, 'w-')\n",
    "            print('%s>%s' % (k, s_k))\n",
    "            second_dict[s_k] = third_search_dict\n",
    "    fin_info_list = get_info_list(search_dict, layer, tmp_list, search_list)\n",
    "    fin_info_pn_list = get_info_pn_list(fin_info_list)\n",
    "    p = Pool(4)  # 设置进程池\n",
    "    assignment_list = assignment_search_list(fin_info_pn_list, 2)  # 分配任务，用于多进程\n",
    "    result = []  # 多进程结果列表\n",
    "    for i in range(len(assignment_list)):\n",
    "        result.append(p.apply_async(get_info, args=(assignment_list[i], i)))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    for result_i in range(len(result)):\n",
    "        fin_info_result_list = result[result_i].get()\n",
    "        fin_save_list.extend(fin_info_result_list)  # 将各个进程获得的列表合并\n",
    "    save_excel(fin_save_list, file_name)\n",
    "    endtime = datetime.datetime.now()\n",
    "    time = (endtime - starttime).seconds\n",
    "    print('总共用时：%s s' % time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
